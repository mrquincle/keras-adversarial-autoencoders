{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code explanation\n",
    "\n",
    "At [this blog](http://nnormandin.com/science/2017/07/01/cvae.html) quite a few details of typical Keras models are explained. Note that older Keras versions had different ways to handle merging of layers as for a variational autoencoder (see e.g. [here](https://github.com/keras-team/keras/issues/3921)).\n",
    "\n",
    "However, I don't get why there is a `sample_z` function. The purpose of an adversarial autoencoder is that it would not need differentiable probability densities in the latent layer, or that's what I thought. The latent representation should be compared to samples from a normal distribution by the discriminator.\n",
    "\n",
    "Ah, that is actually in the original paper! The authors distinguish three different autoencoders. (1) The deterministic autoencoder (that's if you skip the layer containing random variables altogether). (2) An autoencoder that uses a Gaussian posterior. In this case we can indeed use the same renormalization trick as in Kingma and Welling. (3) A general autoencoder with a \"univeral approximate posterior\" where we add noise to the input of the encoder.\n",
    "\n",
    "The network has to match q(z) to p(z) by only exploiting the stochasticity in the data distribution in the deterministic case. However, the authors found that for all different types an extensive sweep over hyperparameters did obtain similiar test-likelihoods. All their reported results where subsequently with a deterministic autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Lambda\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import losses\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of a non-deterministic autoencoder we have a layer with random variables where we sample from using the renormalization trick. See my website on [inference](https://www.annevanrossum.com/blog/2018/01/30/inference-in-deep-learning/) and other [variance reduction methods](https://www.annevanrossum.com/blog/2018/05/26/random-gradients/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_z(args):\n",
    "    mu, log_var = args\n",
    "    batch = K.shape(mu)[0]\n",
    "    eps = K.random_normal(shape=(batch, latent_dim), mean=0., stddev=1.)\n",
    "    return mu + K.exp(log_var / 2) * eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder, discriminator, and decoder have layers of size 512 or 256 and are densely connected. I have not experimented much with the number of nodes. Regarding the activation function leaky rectifiers are used. A rectifier is a function of the form: $f(x) = \\max(0,x)$, in other words, making sure the values don't go below zero, but not bounding it from above. The leaky rectifiers are defined through $f(x) = x$ for $x > 0$ and $f(x) = x \\alpha$ otherwise. This makes it less likely to have them \"stuck\" when all there inputs become negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder(latent_dim, img_shape):\n",
    "    deterministic = 1\n",
    "    img = Input(shape=img_shape)\n",
    "    h = Flatten()(img)\n",
    "    h = Dense(512)(h)\n",
    "    h = LeakyReLU(alpha=0.2)(h)\n",
    "    h = Dense(512)(h)\n",
    "    h = LeakyReLU(alpha=0.2)(h)\n",
    "    if deterministic:\n",
    "        latent_repr = Dense(latent_dim)(h)\n",
    "    else:\n",
    "        mu = Dense(latent_dim)(h)\n",
    "        log_var = Dense(latent_dim)(h)\n",
    "        latent_repr = Lambda(sample_z)([mu, log_var])\n",
    "    return Model(img, latent_repr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(latent_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    encoded_repr = Input(shape=(latent_dim, ))\n",
    "    validity = model(encoded_repr)\n",
    "    return Model(encoded_repr, validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decoder(latent_dim, img_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(np.prod(img_shape), activation='tanh'))\n",
    "    model.add(Reshape(img_shape))\n",
    "    z = Input(shape=(latent_dim,))\n",
    "    img = model(z)\n",
    "    return Model(z, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input are 28x28 images. The optimization used is Adam. The loss is binary cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows = 28\n",
    "img_cols = 28\n",
    "channels = 1\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "# Results can be found in just_2_rv\n",
    "#latent_dim = 2\n",
    "latent_dim = 8\n",
    "\n",
    "optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator(latent_dim)\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the encoder / decoder\n",
    "encoder = build_encoder(latent_dim, img_shape)\n",
    "decoder = build_decoder(latent_dim, img_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The generator takes the image, encodes it and reconstructs it\n",
    "# from the encoding\n",
    "img = Input(shape=img_shape)\n",
    "encoded_repr = encoder(img)\n",
    "reconstructed_img = decoder(encoded_repr)\n",
    "\n",
    "# For the adversarial_autoencoder model we will only train the generator\n",
    "# It will say something like: \n",
    "#   UserWarning: Discrepancy between trainable weights and collected trainable weights, \n",
    "#   did you set `model.trainable` without calling `model.compile` after ?\n",
    "# We only set trainable to false for the discriminator when it is part of the autoencoder...\n",
    "discriminator.trainable = False\n",
    "\n",
    "# The discriminator determines validity of the encoding\n",
    "validity = discriminator(encoded_repr)\n",
    "\n",
    "# The adversarial_autoencoder model  (stacked generator and discriminator)\n",
    "adversarial_autoencoder = Model(img, [reconstructed_img, validity])\n",
    "adversarial_autoencoder.compile(loss=['mse', 'binary_crossentropy'], loss_weights=[0.999, 0.001], optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "sequential_7 (Sequential)    (None, 1)                 136193    \n",
      "=================================================================\n",
      "Total params: 272,386\n",
      "Trainable params: 136,193\n",
      "Non-trainable params: 136,193\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anne/.local/lib/python3.6/site-packages/keras/engine/training.py:478: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    }
   ],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=5000\n",
    "batch_size=128\n",
    "sample_interval=100 \n",
    "    \n",
    "# Load the dataset\n",
    "(X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "# Rescale -1 to 1\n",
    "X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "# Adversarial ground truths\n",
    "valid = np.ones((batch_size, 1))\n",
    "fake = np.zeros((batch_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_prior(batch_size, latent_dim):\n",
    "    return np.random.normal(size=(batch_size, latent_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(latent_dim, decoder, epoch):\n",
    "    r, c = 5, 5\n",
    "\n",
    "    z = sample_prior(r*c, latent_dim)\n",
    "    gen_imgs = decoder.predict(z)\n",
    "\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Each epoch a batch is chosen from the images at random. The typical batch size is 128 items out of 60.000 images. The change to pick the same image is minimal (but not zero).\n",
    "\n",
    "The \"real\" latent variables for the encoder will be Normal distributed. They all have the same N(0,1) distribution, mu=0, sigma=1. The variables are returned in a 128x10 matrix if we use 10 latent variables.\n",
    "\n",
    "The discriminator doesn't know that there is order to the \"real\" and \"fake\" samples. We can just first train it on all the real ones and then all the fake ones. I don't know if it matters for the training, but we might try to actually build one data structure where this is randomized...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anne/.local/lib/python3.6/site-packages/keras/engine/training.py:478: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.733090, acc: 29.30%] [G loss: 0.937058, mse: 0.937302]\n",
      "10 [D loss: 0.239192, acc: 95.70%] [G loss: 0.298136, mse: 0.291328]\n",
      "20 [D loss: 0.144058, acc: 100.00%] [G loss: 0.264847, mse: 0.259248]\n",
      "30 [D loss: 0.092582, acc: 100.00%] [G loss: 0.242276, mse: 0.236328]\n",
      "40 [D loss: 0.074854, acc: 99.61%] [G loss: 0.222630, mse: 0.215970]\n",
      "50 [D loss: 0.062436, acc: 99.22%] [G loss: 0.190846, mse: 0.183588]\n",
      "60 [D loss: 0.095282, acc: 96.09%] [G loss: 0.191600, mse: 0.183074]\n",
      "70 [D loss: 0.148439, acc: 93.36%] [G loss: 0.187342, mse: 0.179397]\n",
      "80 [D loss: 0.282353, acc: 87.11%] [G loss: 0.162460, mse: 0.153044]\n",
      "90 [D loss: 0.272774, acc: 86.33%] [G loss: 0.158903, mse: 0.151126]\n",
      "100 [D loss: 0.414059, acc: 76.95%] [G loss: 0.155098, mse: 0.149622]\n",
      "110 [D loss: 0.311753, acc: 81.64%] [G loss: 0.147203, mse: 0.141764]\n",
      "120 [D loss: 0.284345, acc: 86.33%] [G loss: 0.146561, mse: 0.141146]\n",
      "130 [D loss: 0.231917, acc: 93.36%] [G loss: 0.137570, mse: 0.132817]\n",
      "140 [D loss: 0.170791, acc: 96.88%] [G loss: 0.142926, mse: 0.137890]\n",
      "150 [D loss: 0.175916, acc: 98.44%] [G loss: 0.138126, mse: 0.133399]\n",
      "160 [D loss: 0.132169, acc: 100.00%] [G loss: 0.126025, mse: 0.121304]\n",
      "170 [D loss: 0.109051, acc: 99.61%] [G loss: 0.131107, mse: 0.125811]\n",
      "180 [D loss: 0.090378, acc: 99.61%] [G loss: 0.131926, mse: 0.127060]\n",
      "190 [D loss: 0.089707, acc: 100.00%] [G loss: 0.128308, mse: 0.123232]\n",
      "200 [D loss: 0.064766, acc: 100.00%] [G loss: 0.132475, mse: 0.127256]\n",
      "210 [D loss: 0.054654, acc: 100.00%] [G loss: 0.134273, mse: 0.129085]\n",
      "220 [D loss: 0.050437, acc: 100.00%] [G loss: 0.114724, mse: 0.109235]\n",
      "230 [D loss: 0.042684, acc: 100.00%] [G loss: 0.121486, mse: 0.115665]\n",
      "240 [D loss: 0.037770, acc: 100.00%] [G loss: 0.128650, mse: 0.123118]\n",
      "250 [D loss: 0.038099, acc: 99.61%] [G loss: 0.126458, mse: 0.121010]\n",
      "260 [D loss: 0.030583, acc: 100.00%] [G loss: 0.124146, mse: 0.118256]\n",
      "270 [D loss: 0.030377, acc: 100.00%] [G loss: 0.123146, mse: 0.117467]\n",
      "280 [D loss: 0.031907, acc: 99.61%] [G loss: 0.112452, mse: 0.106048]\n",
      "290 [D loss: 0.025678, acc: 100.00%] [G loss: 0.126780, mse: 0.120582]\n",
      "300 [D loss: 0.030817, acc: 100.00%] [G loss: 0.125619, mse: 0.119802]\n",
      "310 [D loss: 0.024755, acc: 100.00%] [G loss: 0.112942, mse: 0.106835]\n",
      "320 [D loss: 0.025199, acc: 99.61%] [G loss: 0.114164, mse: 0.108016]\n",
      "330 [D loss: 0.023638, acc: 100.00%] [G loss: 0.122690, mse: 0.116344]\n",
      "340 [D loss: 0.026102, acc: 100.00%] [G loss: 0.128305, mse: 0.122029]\n",
      "350 [D loss: 0.018182, acc: 100.00%] [G loss: 0.123392, mse: 0.116869]\n",
      "360 [D loss: 0.019053, acc: 100.00%] [G loss: 0.113023, mse: 0.106382]\n",
      "370 [D loss: 0.018757, acc: 100.00%] [G loss: 0.111376, mse: 0.104870]\n",
      "380 [D loss: 0.017796, acc: 100.00%] [G loss: 0.114783, mse: 0.107836]\n",
      "390 [D loss: 0.020776, acc: 100.00%] [G loss: 0.114446, mse: 0.107892]\n",
      "400 [D loss: 0.014819, acc: 100.00%] [G loss: 0.122361, mse: 0.115564]\n",
      "410 [D loss: 0.017709, acc: 100.00%] [G loss: 0.121036, mse: 0.114123]\n",
      "420 [D loss: 0.020154, acc: 99.61%] [G loss: 0.122198, mse: 0.115298]\n",
      "430 [D loss: 0.013934, acc: 100.00%] [G loss: 0.119414, mse: 0.112658]\n",
      "440 [D loss: 0.013962, acc: 100.00%] [G loss: 0.114238, mse: 0.107061]\n",
      "450 [D loss: 0.020757, acc: 100.00%] [G loss: 0.117549, mse: 0.110963]\n",
      "460 [D loss: 0.022108, acc: 100.00%] [G loss: 0.116725, mse: 0.109876]\n",
      "470 [D loss: 0.030509, acc: 99.22%] [G loss: 0.117645, mse: 0.110692]\n",
      "480 [D loss: 0.024810, acc: 99.61%] [G loss: 0.114321, mse: 0.107379]\n",
      "490 [D loss: 0.026115, acc: 99.22%] [G loss: 0.123864, mse: 0.117040]\n",
      "500 [D loss: 0.025757, acc: 99.61%] [G loss: 0.111312, mse: 0.104557]\n",
      "510 [D loss: 0.020452, acc: 99.61%] [G loss: 0.112408, mse: 0.105217]\n",
      "520 [D loss: 0.025532, acc: 99.22%] [G loss: 0.111775, mse: 0.104777]\n",
      "530 [D loss: 0.023443, acc: 100.00%] [G loss: 0.115307, mse: 0.108507]\n",
      "540 [D loss: 0.040556, acc: 98.83%] [G loss: 0.110620, mse: 0.104293]\n",
      "550 [D loss: 0.040771, acc: 99.22%] [G loss: 0.113989, mse: 0.106967]\n",
      "560 [D loss: 0.042275, acc: 99.22%] [G loss: 0.116157, mse: 0.109519]\n",
      "570 [D loss: 0.051825, acc: 98.44%] [G loss: 0.112898, mse: 0.106325]\n",
      "580 [D loss: 0.042180, acc: 98.44%] [G loss: 0.118744, mse: 0.112190]\n",
      "590 [D loss: 0.043718, acc: 99.22%] [G loss: 0.111776, mse: 0.105179]\n",
      "600 [D loss: 0.056864, acc: 98.05%] [G loss: 0.112893, mse: 0.106751]\n",
      "610 [D loss: 0.046213, acc: 98.83%] [G loss: 0.115690, mse: 0.109294]\n",
      "620 [D loss: 0.035103, acc: 99.22%] [G loss: 0.110604, mse: 0.104017]\n",
      "630 [D loss: 0.062939, acc: 98.05%] [G loss: 0.107162, mse: 0.100078]\n",
      "640 [D loss: 0.057000, acc: 98.05%] [G loss: 0.108614, mse: 0.101929]\n",
      "650 [D loss: 0.076919, acc: 97.27%] [G loss: 0.115194, mse: 0.109190]\n",
      "660 [D loss: 0.064861, acc: 96.88%] [G loss: 0.111884, mse: 0.105359]\n",
      "670 [D loss: 0.065378, acc: 96.88%] [G loss: 0.114942, mse: 0.108831]\n",
      "680 [D loss: 0.070831, acc: 97.27%] [G loss: 0.108167, mse: 0.101749]\n",
      "690 [D loss: 0.074509, acc: 96.88%] [G loss: 0.107024, mse: 0.100751]\n",
      "700 [D loss: 0.071964, acc: 96.88%] [G loss: 0.107371, mse: 0.101448]\n",
      "710 [D loss: 0.101332, acc: 97.27%] [G loss: 0.103168, mse: 0.097537]\n",
      "720 [D loss: 0.082038, acc: 97.66%] [G loss: 0.109153, mse: 0.103054]\n",
      "730 [D loss: 0.136382, acc: 96.48%] [G loss: 0.107550, mse: 0.102369]\n",
      "740 [D loss: 0.136737, acc: 94.53%] [G loss: 0.111786, mse: 0.105879]\n",
      "750 [D loss: 0.148117, acc: 95.31%] [G loss: 0.105199, mse: 0.099915]\n",
      "760 [D loss: 0.122385, acc: 94.92%] [G loss: 0.105386, mse: 0.099760]\n",
      "770 [D loss: 0.190854, acc: 92.19%] [G loss: 0.113664, mse: 0.108673]\n",
      "780 [D loss: 0.166768, acc: 93.36%] [G loss: 0.108159, mse: 0.103172]\n",
      "790 [D loss: 0.138279, acc: 94.92%] [G loss: 0.109509, mse: 0.104356]\n",
      "800 [D loss: 0.097192, acc: 96.48%] [G loss: 0.108589, mse: 0.103114]\n",
      "810 [D loss: 0.236896, acc: 91.80%] [G loss: 0.107192, mse: 0.102223]\n",
      "820 [D loss: 0.227314, acc: 89.45%] [G loss: 0.113061, mse: 0.108434]\n",
      "830 [D loss: 0.272770, acc: 89.45%] [G loss: 0.112706, mse: 0.108573]\n",
      "840 [D loss: 0.184805, acc: 91.80%] [G loss: 0.103563, mse: 0.099030]\n",
      "850 [D loss: 0.216111, acc: 89.84%] [G loss: 0.105852, mse: 0.101624]\n",
      "860 [D loss: 0.270090, acc: 87.50%] [G loss: 0.104347, mse: 0.099969]\n",
      "870 [D loss: 0.249843, acc: 89.84%] [G loss: 0.107381, mse: 0.103214]\n",
      "880 [D loss: 0.268760, acc: 89.06%] [G loss: 0.104685, mse: 0.100909]\n",
      "890 [D loss: 0.256561, acc: 88.67%] [G loss: 0.108232, mse: 0.104337]\n",
      "900 [D loss: 0.259044, acc: 89.45%] [G loss: 0.104007, mse: 0.100287]\n",
      "910 [D loss: 0.379778, acc: 83.98%] [G loss: 0.100679, mse: 0.097198]\n",
      "920 [D loss: 0.281936, acc: 87.50%] [G loss: 0.105316, mse: 0.101594]\n",
      "930 [D loss: 0.332806, acc: 85.94%] [G loss: 0.110616, mse: 0.106978]\n",
      "940 [D loss: 0.304998, acc: 89.45%] [G loss: 0.103793, mse: 0.100299]\n",
      "950 [D loss: 0.337286, acc: 87.89%] [G loss: 0.108958, mse: 0.105396]\n",
      "960 [D loss: 0.280541, acc: 89.84%] [G loss: 0.098971, mse: 0.095419]\n",
      "970 [D loss: 0.365635, acc: 83.59%] [G loss: 0.106043, mse: 0.102713]\n",
      "980 [D loss: 0.324247, acc: 84.38%] [G loss: 0.105594, mse: 0.102342]\n",
      "990 [D loss: 0.328071, acc: 85.55%] [G loss: 0.095970, mse: 0.092702]\n",
      "1000 [D loss: 0.342652, acc: 86.33%] [G loss: 0.094554, mse: 0.091346]\n",
      "1010 [D loss: 0.313464, acc: 88.28%] [G loss: 0.094921, mse: 0.091376]\n",
      "1020 [D loss: 0.373910, acc: 83.20%] [G loss: 0.107910, mse: 0.104885]\n",
      "1030 [D loss: 0.330745, acc: 86.72%] [G loss: 0.102165, mse: 0.099291]\n",
      "1040 [D loss: 0.309928, acc: 85.55%] [G loss: 0.100825, mse: 0.097584]\n",
      "1050 [D loss: 0.335533, acc: 86.33%] [G loss: 0.095691, mse: 0.092527]\n",
      "1060 [D loss: 0.340418, acc: 85.16%] [G loss: 0.103773, mse: 0.100840]\n",
      "1070 [D loss: 0.351408, acc: 83.59%] [G loss: 0.097160, mse: 0.094304]\n",
      "1080 [D loss: 0.320442, acc: 87.89%] [G loss: 0.103871, mse: 0.100803]\n",
      "1090 [D loss: 0.313346, acc: 88.67%] [G loss: 0.105055, mse: 0.102279]\n",
      "1100 [D loss: 0.298666, acc: 88.28%] [G loss: 0.092387, mse: 0.089256]\n",
      "1110 [D loss: 0.323018, acc: 87.89%] [G loss: 0.097900, mse: 0.095038]\n",
      "1120 [D loss: 0.433920, acc: 81.64%] [G loss: 0.102648, mse: 0.100314]\n",
      "1130 [D loss: 0.235937, acc: 91.02%] [G loss: 0.097191, mse: 0.094302]\n",
      "1140 [D loss: 0.322339, acc: 87.11%] [G loss: 0.093266, mse: 0.090488]\n",
      "1150 [D loss: 0.386777, acc: 85.55%] [G loss: 0.102152, mse: 0.099353]\n",
      "1160 [D loss: 0.361089, acc: 84.77%] [G loss: 0.097026, mse: 0.094688]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1170 [D loss: 0.381582, acc: 82.81%] [G loss: 0.095675, mse: 0.093037]\n",
      "1180 [D loss: 0.448240, acc: 81.64%] [G loss: 0.098654, mse: 0.096297]\n",
      "1190 [D loss: 0.414639, acc: 82.42%] [G loss: 0.106431, mse: 0.104133]\n",
      "1200 [D loss: 0.366467, acc: 82.42%] [G loss: 0.100071, mse: 0.097311]\n",
      "1210 [D loss: 0.464068, acc: 82.42%] [G loss: 0.095387, mse: 0.092945]\n",
      "1220 [D loss: 0.390966, acc: 83.98%] [G loss: 0.096915, mse: 0.094313]\n",
      "1230 [D loss: 0.323465, acc: 84.38%] [G loss: 0.099023, mse: 0.096262]\n",
      "1240 [D loss: 0.415529, acc: 81.64%] [G loss: 0.096245, mse: 0.093837]\n",
      "1250 [D loss: 0.441138, acc: 80.47%] [G loss: 0.103170, mse: 0.101083]\n",
      "1260 [D loss: 0.394912, acc: 84.77%] [G loss: 0.086904, mse: 0.084378]\n",
      "1270 [D loss: 0.306187, acc: 88.28%] [G loss: 0.090949, mse: 0.088351]\n",
      "1280 [D loss: 0.460602, acc: 81.64%] [G loss: 0.098537, mse: 0.096169]\n",
      "1290 [D loss: 0.426463, acc: 81.64%] [G loss: 0.100142, mse: 0.097736]\n",
      "1300 [D loss: 0.401576, acc: 82.81%] [G loss: 0.090307, mse: 0.087849]\n",
      "1310 [D loss: 0.429987, acc: 81.25%] [G loss: 0.090106, mse: 0.087664]\n",
      "1320 [D loss: 0.363858, acc: 84.77%] [G loss: 0.093482, mse: 0.091243]\n",
      "1330 [D loss: 0.419852, acc: 80.86%] [G loss: 0.087933, mse: 0.085679]\n",
      "1340 [D loss: 0.396270, acc: 81.64%] [G loss: 0.091607, mse: 0.089249]\n",
      "1350 [D loss: 0.457257, acc: 78.91%] [G loss: 0.097974, mse: 0.095892]\n",
      "1360 [D loss: 0.453340, acc: 80.86%] [G loss: 0.098094, mse: 0.095921]\n",
      "1370 [D loss: 0.430979, acc: 79.30%] [G loss: 0.095703, mse: 0.093706]\n",
      "1380 [D loss: 0.387765, acc: 81.64%] [G loss: 0.100020, mse: 0.097893]\n",
      "1390 [D loss: 0.398465, acc: 85.55%] [G loss: 0.091779, mse: 0.089768]\n",
      "1400 [D loss: 0.480299, acc: 76.95%] [G loss: 0.091789, mse: 0.090038]\n",
      "1410 [D loss: 0.454389, acc: 79.30%] [G loss: 0.097786, mse: 0.095569]\n",
      "1420 [D loss: 0.420963, acc: 82.03%] [G loss: 0.089095, mse: 0.087111]\n",
      "1430 [D loss: 0.373216, acc: 85.55%] [G loss: 0.092337, mse: 0.090044]\n",
      "1440 [D loss: 0.348579, acc: 86.33%] [G loss: 0.089574, mse: 0.087154]\n",
      "1450 [D loss: 0.489544, acc: 78.52%] [G loss: 0.099357, mse: 0.097524]\n",
      "1460 [D loss: 0.476908, acc: 78.12%] [G loss: 0.096323, mse: 0.094588]\n",
      "1470 [D loss: 0.405146, acc: 80.86%] [G loss: 0.099760, mse: 0.097771]\n",
      "1480 [D loss: 0.428662, acc: 80.08%] [G loss: 0.092431, mse: 0.090276]\n",
      "1490 [D loss: 0.367762, acc: 83.20%] [G loss: 0.095239, mse: 0.093079]\n",
      "1500 [D loss: 0.408400, acc: 83.20%] [G loss: 0.091409, mse: 0.089342]\n",
      "1510 [D loss: 0.470297, acc: 80.86%] [G loss: 0.096685, mse: 0.094877]\n",
      "1520 [D loss: 0.428347, acc: 81.25%] [G loss: 0.090660, mse: 0.088579]\n",
      "1530 [D loss: 0.410480, acc: 82.42%] [G loss: 0.093504, mse: 0.091501]\n",
      "1540 [D loss: 0.376084, acc: 83.59%] [G loss: 0.095060, mse: 0.092914]\n",
      "1550 [D loss: 0.462655, acc: 81.64%] [G loss: 0.100038, mse: 0.098139]\n",
      "1560 [D loss: 0.420364, acc: 81.64%] [G loss: 0.093754, mse: 0.091839]\n",
      "1570 [D loss: 0.402855, acc: 82.03%] [G loss: 0.091983, mse: 0.089856]\n",
      "1580 [D loss: 0.366126, acc: 84.77%] [G loss: 0.086845, mse: 0.084885]\n",
      "1590 [D loss: 0.386001, acc: 83.59%] [G loss: 0.085902, mse: 0.083869]\n",
      "1600 [D loss: 0.431863, acc: 78.12%] [G loss: 0.090788, mse: 0.088751]\n",
      "1610 [D loss: 0.463602, acc: 76.95%] [G loss: 0.088372, mse: 0.086529]\n",
      "1620 [D loss: 0.496460, acc: 76.56%] [G loss: 0.093957, mse: 0.092050]\n",
      "1630 [D loss: 0.400456, acc: 82.42%] [G loss: 0.083722, mse: 0.081780]\n",
      "1640 [D loss: 0.482372, acc: 78.12%] [G loss: 0.095395, mse: 0.093639]\n",
      "1650 [D loss: 0.448341, acc: 77.73%] [G loss: 0.099531, mse: 0.097893]\n",
      "1660 [D loss: 0.488592, acc: 78.91%] [G loss: 0.097446, mse: 0.095755]\n",
      "1670 [D loss: 0.536541, acc: 76.56%] [G loss: 0.097759, mse: 0.096010]\n",
      "1680 [D loss: 0.357283, acc: 85.16%] [G loss: 0.083507, mse: 0.081594]\n",
      "1690 [D loss: 0.434391, acc: 83.20%] [G loss: 0.088605, mse: 0.086765]\n",
      "1700 [D loss: 0.469143, acc: 77.73%] [G loss: 0.098544, mse: 0.096953]\n",
      "1710 [D loss: 0.403490, acc: 81.64%] [G loss: 0.086389, mse: 0.084436]\n",
      "1720 [D loss: 0.482190, acc: 77.34%] [G loss: 0.101740, mse: 0.100072]\n",
      "1730 [D loss: 0.442616, acc: 81.25%] [G loss: 0.093525, mse: 0.091597]\n",
      "1740 [D loss: 0.434861, acc: 80.86%] [G loss: 0.094336, mse: 0.092415]\n",
      "1750 [D loss: 0.497864, acc: 76.95%] [G loss: 0.095069, mse: 0.093399]\n",
      "1760 [D loss: 0.507137, acc: 77.34%] [G loss: 0.090696, mse: 0.088945]\n",
      "1770 [D loss: 0.423858, acc: 81.25%] [G loss: 0.084041, mse: 0.082258]\n",
      "1780 [D loss: 0.438682, acc: 80.86%] [G loss: 0.086230, mse: 0.084590]\n",
      "1790 [D loss: 0.427583, acc: 81.64%] [G loss: 0.088406, mse: 0.086423]\n",
      "1800 [D loss: 0.438065, acc: 81.25%] [G loss: 0.089889, mse: 0.088122]\n",
      "1810 [D loss: 0.477366, acc: 79.30%] [G loss: 0.086981, mse: 0.085291]\n",
      "1820 [D loss: 0.482012, acc: 77.34%] [G loss: 0.085101, mse: 0.083342]\n",
      "1830 [D loss: 0.484773, acc: 76.95%] [G loss: 0.093346, mse: 0.091709]\n",
      "1840 [D loss: 0.435297, acc: 81.64%] [G loss: 0.090472, mse: 0.088656]\n",
      "1850 [D loss: 0.469652, acc: 76.17%] [G loss: 0.090020, mse: 0.088239]\n",
      "1860 [D loss: 0.420158, acc: 80.86%] [G loss: 0.087853, mse: 0.085940]\n",
      "1870 [D loss: 0.535114, acc: 73.05%] [G loss: 0.089393, mse: 0.087719]\n",
      "1880 [D loss: 0.554809, acc: 76.95%] [G loss: 0.096314, mse: 0.094770]\n",
      "1890 [D loss: 0.475399, acc: 80.86%] [G loss: 0.102029, mse: 0.100386]\n",
      "1900 [D loss: 0.450743, acc: 78.52%] [G loss: 0.093765, mse: 0.092008]\n",
      "1910 [D loss: 0.453541, acc: 77.73%] [G loss: 0.089385, mse: 0.087668]\n",
      "1920 [D loss: 0.432129, acc: 80.86%] [G loss: 0.088796, mse: 0.087035]\n",
      "1930 [D loss: 0.393671, acc: 83.59%] [G loss: 0.085723, mse: 0.083888]\n",
      "1940 [D loss: 0.427187, acc: 82.42%] [G loss: 0.085304, mse: 0.083522]\n",
      "1950 [D loss: 0.463093, acc: 80.08%] [G loss: 0.088847, mse: 0.087134]\n",
      "1960 [D loss: 0.515430, acc: 74.61%] [G loss: 0.086746, mse: 0.085151]\n",
      "1970 [D loss: 0.517504, acc: 77.34%] [G loss: 0.091769, mse: 0.090147]\n",
      "1980 [D loss: 0.486663, acc: 78.91%] [G loss: 0.091726, mse: 0.090130]\n",
      "1990 [D loss: 0.471217, acc: 78.91%] [G loss: 0.088421, mse: 0.086730]\n",
      "2000 [D loss: 0.476764, acc: 78.12%] [G loss: 0.093095, mse: 0.091417]\n",
      "2010 [D loss: 0.441788, acc: 83.20%] [G loss: 0.083372, mse: 0.081612]\n",
      "2020 [D loss: 0.438179, acc: 80.08%] [G loss: 0.079420, mse: 0.077752]\n",
      "2030 [D loss: 0.470792, acc: 80.47%] [G loss: 0.092449, mse: 0.090978]\n",
      "2040 [D loss: 0.440574, acc: 80.47%] [G loss: 0.081334, mse: 0.079600]\n",
      "2050 [D loss: 0.485456, acc: 75.00%] [G loss: 0.092711, mse: 0.091220]\n",
      "2060 [D loss: 0.423207, acc: 80.86%] [G loss: 0.082947, mse: 0.081334]\n",
      "2070 [D loss: 0.468880, acc: 80.86%] [G loss: 0.088878, mse: 0.087395]\n",
      "2080 [D loss: 0.435162, acc: 81.25%] [G loss: 0.089695, mse: 0.088178]\n",
      "2090 [D loss: 0.564096, acc: 74.22%] [G loss: 0.089917, mse: 0.088294]\n",
      "2100 [D loss: 0.436004, acc: 82.81%] [G loss: 0.086466, mse: 0.084835]\n",
      "2110 [D loss: 0.466378, acc: 75.78%] [G loss: 0.086512, mse: 0.084882]\n",
      "2120 [D loss: 0.524536, acc: 76.95%] [G loss: 0.089669, mse: 0.088209]\n",
      "2130 [D loss: 0.460537, acc: 81.25%] [G loss: 0.083304, mse: 0.081701]\n",
      "2140 [D loss: 0.463834, acc: 77.34%] [G loss: 0.087964, mse: 0.086413]\n",
      "2150 [D loss: 0.449344, acc: 78.52%] [G loss: 0.083960, mse: 0.082283]\n",
      "2160 [D loss: 0.620307, acc: 72.27%] [G loss: 0.088385, mse: 0.086968]\n",
      "2170 [D loss: 0.487279, acc: 77.34%] [G loss: 0.090503, mse: 0.088964]\n",
      "2180 [D loss: 0.445964, acc: 81.25%] [G loss: 0.084664, mse: 0.083251]\n",
      "2190 [D loss: 0.474191, acc: 78.52%] [G loss: 0.091506, mse: 0.089955]\n",
      "2200 [D loss: 0.445642, acc: 82.03%] [G loss: 0.087956, mse: 0.086373]\n",
      "2210 [D loss: 0.474892, acc: 76.95%] [G loss: 0.085892, mse: 0.084322]\n",
      "2220 [D loss: 0.428323, acc: 80.08%] [G loss: 0.082382, mse: 0.080687]\n",
      "2230 [D loss: 0.449281, acc: 79.69%] [G loss: 0.090458, mse: 0.088841]\n",
      "2240 [D loss: 0.461291, acc: 81.25%] [G loss: 0.083441, mse: 0.081741]\n",
      "2250 [D loss: 0.454348, acc: 79.69%] [G loss: 0.084196, mse: 0.082704]\n",
      "2260 [D loss: 0.500456, acc: 76.17%] [G loss: 0.089034, mse: 0.087434]\n",
      "2270 [D loss: 0.438479, acc: 79.69%] [G loss: 0.089139, mse: 0.087526]\n",
      "2280 [D loss: 0.486481, acc: 76.56%] [G loss: 0.080290, mse: 0.078516]\n",
      "2290 [D loss: 0.515907, acc: 73.83%] [G loss: 0.087341, mse: 0.085775]\n",
      "2300 [D loss: 0.491331, acc: 76.95%] [G loss: 0.084087, mse: 0.082615]\n",
      "2310 [D loss: 0.425914, acc: 82.42%] [G loss: 0.087110, mse: 0.085362]\n",
      "2320 [D loss: 0.402257, acc: 84.77%] [G loss: 0.079776, mse: 0.078024]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2330 [D loss: 0.429680, acc: 85.16%] [G loss: 0.088079, mse: 0.086561]\n",
      "2340 [D loss: 0.548557, acc: 75.39%] [G loss: 0.087695, mse: 0.086288]\n",
      "2350 [D loss: 0.386990, acc: 85.16%] [G loss: 0.078010, mse: 0.076290]\n",
      "2360 [D loss: 0.509312, acc: 74.22%] [G loss: 0.083256, mse: 0.081769]\n",
      "2370 [D loss: 0.515790, acc: 75.39%] [G loss: 0.090319, mse: 0.088793]\n",
      "2380 [D loss: 0.422168, acc: 82.42%] [G loss: 0.081264, mse: 0.079667]\n",
      "2390 [D loss: 0.506212, acc: 76.95%] [G loss: 0.089530, mse: 0.088181]\n",
      "2400 [D loss: 0.510352, acc: 75.39%] [G loss: 0.086726, mse: 0.085118]\n",
      "2410 [D loss: 0.472916, acc: 79.69%] [G loss: 0.092049, mse: 0.090543]\n",
      "2420 [D loss: 0.511076, acc: 75.39%] [G loss: 0.092242, mse: 0.090883]\n",
      "2430 [D loss: 0.535683, acc: 76.17%] [G loss: 0.084046, mse: 0.082635]\n",
      "2440 [D loss: 0.507388, acc: 77.34%] [G loss: 0.083620, mse: 0.082116]\n",
      "2450 [D loss: 0.438597, acc: 79.69%] [G loss: 0.082667, mse: 0.080906]\n",
      "2460 [D loss: 0.458207, acc: 82.42%] [G loss: 0.082713, mse: 0.081152]\n",
      "2470 [D loss: 0.514188, acc: 76.17%] [G loss: 0.081366, mse: 0.079769]\n",
      "2480 [D loss: 0.436946, acc: 78.52%] [G loss: 0.081385, mse: 0.079889]\n",
      "2490 [D loss: 0.476120, acc: 77.73%] [G loss: 0.083005, mse: 0.081444]\n",
      "2500 [D loss: 0.450270, acc: 80.47%] [G loss: 0.082244, mse: 0.080550]\n",
      "2510 [D loss: 0.504636, acc: 75.00%] [G loss: 0.089552, mse: 0.088165]\n",
      "2520 [D loss: 0.471983, acc: 74.22%] [G loss: 0.093453, mse: 0.091824]\n",
      "2530 [D loss: 0.434486, acc: 81.25%] [G loss: 0.080644, mse: 0.079015]\n",
      "2540 [D loss: 0.522252, acc: 74.22%] [G loss: 0.088588, mse: 0.087226]\n",
      "2550 [D loss: 0.431187, acc: 80.47%] [G loss: 0.085477, mse: 0.083788]\n",
      "2560 [D loss: 0.500148, acc: 75.00%] [G loss: 0.088066, mse: 0.086609]\n",
      "2570 [D loss: 0.457472, acc: 77.73%] [G loss: 0.084884, mse: 0.083310]\n",
      "2580 [D loss: 0.544597, acc: 71.88%] [G loss: 0.081336, mse: 0.079914]\n",
      "2590 [D loss: 0.477007, acc: 78.52%] [G loss: 0.083637, mse: 0.082065]\n",
      "2600 [D loss: 0.480906, acc: 78.91%] [G loss: 0.086531, mse: 0.085096]\n",
      "2610 [D loss: 0.477293, acc: 74.22%] [G loss: 0.079216, mse: 0.077660]\n",
      "2620 [D loss: 0.472246, acc: 76.17%] [G loss: 0.076493, mse: 0.074885]\n",
      "2630 [D loss: 0.493705, acc: 76.56%] [G loss: 0.081173, mse: 0.079696]\n",
      "2640 [D loss: 0.538802, acc: 73.05%] [G loss: 0.085171, mse: 0.083735]\n",
      "2650 [D loss: 0.528157, acc: 74.22%] [G loss: 0.086231, mse: 0.084834]\n",
      "2660 [D loss: 0.479972, acc: 79.30%] [G loss: 0.083386, mse: 0.081774]\n",
      "2670 [D loss: 0.428503, acc: 80.08%] [G loss: 0.081636, mse: 0.080076]\n",
      "2680 [D loss: 0.500561, acc: 72.66%] [G loss: 0.085139, mse: 0.083784]\n",
      "2690 [D loss: 0.455450, acc: 80.08%] [G loss: 0.079181, mse: 0.077576]\n",
      "2700 [D loss: 0.487108, acc: 76.17%] [G loss: 0.081896, mse: 0.080356]\n",
      "2710 [D loss: 0.530676, acc: 73.05%] [G loss: 0.082501, mse: 0.081065]\n",
      "2720 [D loss: 0.500157, acc: 73.83%] [G loss: 0.084309, mse: 0.082901]\n",
      "2730 [D loss: 0.490620, acc: 76.17%] [G loss: 0.091329, mse: 0.090071]\n",
      "2740 [D loss: 0.481003, acc: 78.12%] [G loss: 0.083029, mse: 0.081504]\n",
      "2750 [D loss: 0.509957, acc: 77.73%] [G loss: 0.082192, mse: 0.080759]\n",
      "2760 [D loss: 0.454035, acc: 78.12%] [G loss: 0.072611, mse: 0.071100]\n",
      "2770 [D loss: 0.475455, acc: 78.52%] [G loss: 0.080740, mse: 0.079248]\n",
      "2780 [D loss: 0.539467, acc: 73.83%] [G loss: 0.083633, mse: 0.082223]\n",
      "2790 [D loss: 0.457418, acc: 80.08%] [G loss: 0.085978, mse: 0.084525]\n",
      "2800 [D loss: 0.461525, acc: 78.12%] [G loss: 0.080701, mse: 0.079297]\n",
      "2810 [D loss: 0.489354, acc: 80.08%] [G loss: 0.081007, mse: 0.079601]\n",
      "2820 [D loss: 0.480944, acc: 76.95%] [G loss: 0.088150, mse: 0.086782]\n",
      "2830 [D loss: 0.514279, acc: 73.83%] [G loss: 0.077439, mse: 0.076001]\n",
      "2840 [D loss: 0.511620, acc: 76.95%] [G loss: 0.078641, mse: 0.077318]\n",
      "2850 [D loss: 0.461062, acc: 77.73%] [G loss: 0.080310, mse: 0.078750]\n",
      "2860 [D loss: 0.511434, acc: 75.00%] [G loss: 0.078741, mse: 0.077239]\n",
      "2870 [D loss: 0.558899, acc: 74.61%] [G loss: 0.082584, mse: 0.081172]\n",
      "2880 [D loss: 0.485670, acc: 76.17%] [G loss: 0.084670, mse: 0.083276]\n",
      "2890 [D loss: 0.473642, acc: 73.05%] [G loss: 0.082419, mse: 0.080859]\n",
      "2900 [D loss: 0.463116, acc: 78.12%] [G loss: 0.082938, mse: 0.081540]\n",
      "2910 [D loss: 0.523566, acc: 74.61%] [G loss: 0.081405, mse: 0.079944]\n",
      "2920 [D loss: 0.481695, acc: 74.61%] [G loss: 0.078103, mse: 0.076572]\n",
      "2930 [D loss: 0.505609, acc: 76.17%] [G loss: 0.085789, mse: 0.084407]\n",
      "2940 [D loss: 0.511178, acc: 77.34%] [G loss: 0.084610, mse: 0.083141]\n",
      "2950 [D loss: 0.527032, acc: 74.61%] [G loss: 0.083802, mse: 0.082538]\n",
      "2960 [D loss: 0.449761, acc: 80.86%] [G loss: 0.081127, mse: 0.079508]\n",
      "2970 [D loss: 0.502898, acc: 77.73%] [G loss: 0.083928, mse: 0.082630]\n",
      "2980 [D loss: 0.504982, acc: 76.17%] [G loss: 0.082862, mse: 0.081441]\n",
      "2990 [D loss: 0.485904, acc: 77.73%] [G loss: 0.087474, mse: 0.086122]\n",
      "3000 [D loss: 0.472777, acc: 77.34%] [G loss: 0.086426, mse: 0.084956]\n",
      "3010 [D loss: 0.531387, acc: 73.05%] [G loss: 0.086039, mse: 0.084696]\n",
      "3020 [D loss: 0.470091, acc: 79.30%] [G loss: 0.076629, mse: 0.075173]\n",
      "3030 [D loss: 0.454397, acc: 80.86%] [G loss: 0.080594, mse: 0.079193]\n",
      "3040 [D loss: 0.482748, acc: 81.25%] [G loss: 0.080168, mse: 0.078804]\n",
      "3050 [D loss: 0.502759, acc: 76.56%] [G loss: 0.087589, mse: 0.086139]\n",
      "3060 [D loss: 0.529966, acc: 73.83%] [G loss: 0.081218, mse: 0.079650]\n",
      "3070 [D loss: 0.496096, acc: 78.52%] [G loss: 0.085507, mse: 0.084059]\n",
      "3080 [D loss: 0.479283, acc: 78.91%] [G loss: 0.078976, mse: 0.077518]\n",
      "3090 [D loss: 0.491772, acc: 76.95%] [G loss: 0.084062, mse: 0.082605]\n",
      "3100 [D loss: 0.453040, acc: 81.25%] [G loss: 0.084019, mse: 0.082455]\n",
      "3110 [D loss: 0.495655, acc: 73.44%] [G loss: 0.082991, mse: 0.081735]\n",
      "3120 [D loss: 0.470034, acc: 77.73%] [G loss: 0.081667, mse: 0.080184]\n",
      "3130 [D loss: 0.517737, acc: 73.44%] [G loss: 0.087887, mse: 0.086527]\n",
      "3140 [D loss: 0.483702, acc: 77.34%] [G loss: 0.080202, mse: 0.078691]\n",
      "3150 [D loss: 0.481231, acc: 74.61%] [G loss: 0.086848, mse: 0.085459]\n",
      "3160 [D loss: 0.472341, acc: 80.08%] [G loss: 0.080354, mse: 0.078853]\n",
      "3170 [D loss: 0.544108, acc: 74.61%] [G loss: 0.083766, mse: 0.082457]\n",
      "3180 [D loss: 0.462918, acc: 79.30%] [G loss: 0.073961, mse: 0.072392]\n",
      "3190 [D loss: 0.476328, acc: 78.91%] [G loss: 0.077172, mse: 0.075694]\n",
      "3200 [D loss: 0.532155, acc: 74.22%] [G loss: 0.076144, mse: 0.074729]\n",
      "3210 [D loss: 0.437009, acc: 80.86%] [G loss: 0.079924, mse: 0.078515]\n",
      "3220 [D loss: 0.457219, acc: 78.91%] [G loss: 0.079678, mse: 0.078231]\n",
      "3230 [D loss: 0.463863, acc: 80.47%] [G loss: 0.075263, mse: 0.073886]\n",
      "3240 [D loss: 0.492162, acc: 76.95%] [G loss: 0.072584, mse: 0.071088]\n",
      "3250 [D loss: 0.503538, acc: 76.17%] [G loss: 0.079849, mse: 0.078544]\n",
      "3260 [D loss: 0.531969, acc: 73.83%] [G loss: 0.085442, mse: 0.083988]\n",
      "3270 [D loss: 0.510582, acc: 73.83%] [G loss: 0.079055, mse: 0.077694]\n",
      "3280 [D loss: 0.451587, acc: 78.91%] [G loss: 0.077510, mse: 0.076084]\n",
      "3290 [D loss: 0.480374, acc: 76.17%] [G loss: 0.081790, mse: 0.080435]\n",
      "3300 [D loss: 0.471005, acc: 79.30%] [G loss: 0.074813, mse: 0.073304]\n",
      "3310 [D loss: 0.494102, acc: 78.91%] [G loss: 0.080176, mse: 0.078817]\n",
      "3320 [D loss: 0.502657, acc: 73.83%] [G loss: 0.080496, mse: 0.079167]\n",
      "3330 [D loss: 0.490816, acc: 76.95%] [G loss: 0.078779, mse: 0.077277]\n",
      "3340 [D loss: 0.504762, acc: 77.34%] [G loss: 0.081868, mse: 0.080426]\n",
      "3350 [D loss: 0.484163, acc: 76.95%] [G loss: 0.079853, mse: 0.078532]\n",
      "3360 [D loss: 0.467946, acc: 77.73%] [G loss: 0.077251, mse: 0.075732]\n",
      "3370 [D loss: 0.543749, acc: 73.05%] [G loss: 0.078771, mse: 0.077333]\n",
      "3380 [D loss: 0.501743, acc: 76.56%] [G loss: 0.077642, mse: 0.076191]\n",
      "3390 [D loss: 0.499780, acc: 73.83%] [G loss: 0.080029, mse: 0.078666]\n",
      "3400 [D loss: 0.484547, acc: 78.91%] [G loss: 0.074887, mse: 0.073400]\n",
      "3410 [D loss: 0.487379, acc: 78.52%] [G loss: 0.081394, mse: 0.079986]\n",
      "3420 [D loss: 0.483585, acc: 76.56%] [G loss: 0.078025, mse: 0.076520]\n",
      "3430 [D loss: 0.528280, acc: 71.88%] [G loss: 0.080703, mse: 0.079378]\n",
      "3440 [D loss: 0.544435, acc: 68.75%] [G loss: 0.083468, mse: 0.082199]\n",
      "3450 [D loss: 0.506244, acc: 73.83%] [G loss: 0.079510, mse: 0.078077]\n",
      "3460 [D loss: 0.461262, acc: 81.25%] [G loss: 0.084251, mse: 0.082796]\n",
      "3470 [D loss: 0.513296, acc: 76.17%] [G loss: 0.082211, mse: 0.080946]\n",
      "3480 [D loss: 0.488201, acc: 78.52%] [G loss: 0.080520, mse: 0.079095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3490 [D loss: 0.520496, acc: 77.73%] [G loss: 0.078522, mse: 0.077175]\n",
      "3500 [D loss: 0.472371, acc: 78.52%] [G loss: 0.081553, mse: 0.080117]\n",
      "3510 [D loss: 0.424223, acc: 79.69%] [G loss: 0.076715, mse: 0.075104]\n",
      "3520 [D loss: 0.500713, acc: 76.17%] [G loss: 0.090787, mse: 0.089459]\n",
      "3530 [D loss: 0.465267, acc: 78.91%] [G loss: 0.077123, mse: 0.075528]\n",
      "3540 [D loss: 0.490207, acc: 77.34%] [G loss: 0.080588, mse: 0.079202]\n",
      "3550 [D loss: 0.485929, acc: 78.12%] [G loss: 0.077616, mse: 0.076295]\n",
      "3560 [D loss: 0.495565, acc: 77.34%] [G loss: 0.079649, mse: 0.078177]\n",
      "3570 [D loss: 0.501225, acc: 76.95%] [G loss: 0.082814, mse: 0.081527]\n",
      "3580 [D loss: 0.467843, acc: 78.52%] [G loss: 0.082501, mse: 0.081169]\n",
      "3590 [D loss: 0.483830, acc: 76.56%] [G loss: 0.082574, mse: 0.081120]\n",
      "3600 [D loss: 0.518891, acc: 75.00%] [G loss: 0.083353, mse: 0.082085]\n",
      "3610 [D loss: 0.518961, acc: 73.83%] [G loss: 0.079603, mse: 0.078159]\n",
      "3620 [D loss: 0.539621, acc: 73.83%] [G loss: 0.084420, mse: 0.083170]\n",
      "3630 [D loss: 0.457765, acc: 79.30%] [G loss: 0.075099, mse: 0.073674]\n",
      "3640 [D loss: 0.452642, acc: 78.91%] [G loss: 0.074764, mse: 0.073246]\n",
      "3650 [D loss: 0.464489, acc: 78.52%] [G loss: 0.075383, mse: 0.073896]\n",
      "3660 [D loss: 0.483609, acc: 78.52%] [G loss: 0.080722, mse: 0.079282]\n",
      "3670 [D loss: 0.517123, acc: 73.83%] [G loss: 0.075315, mse: 0.073974]\n",
      "3680 [D loss: 0.491635, acc: 77.34%] [G loss: 0.077101, mse: 0.075727]\n",
      "3690 [D loss: 0.504686, acc: 76.95%] [G loss: 0.081347, mse: 0.079941]\n",
      "3700 [D loss: 0.523099, acc: 76.95%] [G loss: 0.083293, mse: 0.082012]\n",
      "3710 [D loss: 0.460545, acc: 80.47%] [G loss: 0.078256, mse: 0.076792]\n",
      "3720 [D loss: 0.521825, acc: 76.95%] [G loss: 0.078753, mse: 0.077295]\n",
      "3730 [D loss: 0.427063, acc: 80.86%] [G loss: 0.078472, mse: 0.076923]\n",
      "3740 [D loss: 0.504678, acc: 75.39%] [G loss: 0.075775, mse: 0.074480]\n",
      "3750 [D loss: 0.440811, acc: 80.08%] [G loss: 0.079625, mse: 0.078040]\n",
      "3760 [D loss: 0.476646, acc: 78.91%] [G loss: 0.082872, mse: 0.081498]\n",
      "3770 [D loss: 0.422634, acc: 84.38%] [G loss: 0.072436, mse: 0.070984]\n",
      "3780 [D loss: 0.472894, acc: 76.17%] [G loss: 0.077618, mse: 0.076302]\n",
      "3790 [D loss: 0.446259, acc: 81.25%] [G loss: 0.078403, mse: 0.076975]\n",
      "3800 [D loss: 0.478645, acc: 77.34%] [G loss: 0.081081, mse: 0.079621]\n",
      "3810 [D loss: 0.474890, acc: 78.91%] [G loss: 0.072594, mse: 0.071135]\n",
      "3820 [D loss: 0.448412, acc: 78.91%] [G loss: 0.079958, mse: 0.078542]\n",
      "3830 [D loss: 0.445257, acc: 81.64%] [G loss: 0.078579, mse: 0.077078]\n",
      "3840 [D loss: 0.516819, acc: 74.22%] [G loss: 0.083017, mse: 0.081602]\n",
      "3850 [D loss: 0.456598, acc: 75.78%] [G loss: 0.072721, mse: 0.071271]\n",
      "3860 [D loss: 0.462073, acc: 80.47%] [G loss: 0.079127, mse: 0.077573]\n",
      "3870 [D loss: 0.510975, acc: 76.56%] [G loss: 0.078879, mse: 0.077628]\n",
      "3880 [D loss: 0.466159, acc: 78.52%] [G loss: 0.075002, mse: 0.073627]\n",
      "3890 [D loss: 0.535469, acc: 73.83%] [G loss: 0.080953, mse: 0.079589]\n",
      "3900 [D loss: 0.463046, acc: 78.12%] [G loss: 0.076612, mse: 0.075142]\n",
      "3910 [D loss: 0.506458, acc: 77.34%] [G loss: 0.082671, mse: 0.081231]\n",
      "3920 [D loss: 0.441417, acc: 81.25%] [G loss: 0.070737, mse: 0.069320]\n",
      "3930 [D loss: 0.466378, acc: 77.73%] [G loss: 0.078779, mse: 0.077346]\n",
      "3940 [D loss: 0.494895, acc: 78.91%] [G loss: 0.078941, mse: 0.077469]\n",
      "3950 [D loss: 0.478927, acc: 77.73%] [G loss: 0.074116, mse: 0.072575]\n",
      "3960 [D loss: 0.452929, acc: 78.91%] [G loss: 0.071622, mse: 0.070184]\n",
      "3970 [D loss: 0.507924, acc: 76.95%] [G loss: 0.078740, mse: 0.077365]\n",
      "3980 [D loss: 0.500238, acc: 76.95%] [G loss: 0.072639, mse: 0.071264]\n",
      "3990 [D loss: 0.453849, acc: 80.08%] [G loss: 0.071047, mse: 0.069536]\n",
      "4000 [D loss: 0.493214, acc: 77.34%] [G loss: 0.070399, mse: 0.068968]\n",
      "4010 [D loss: 0.451478, acc: 79.30%] [G loss: 0.079603, mse: 0.078135]\n",
      "4020 [D loss: 0.424704, acc: 81.25%] [G loss: 0.071603, mse: 0.070096]\n",
      "4030 [D loss: 0.484435, acc: 76.95%] [G loss: 0.075283, mse: 0.073934]\n",
      "4040 [D loss: 0.496123, acc: 77.34%] [G loss: 0.076718, mse: 0.075193]\n",
      "4050 [D loss: 0.491038, acc: 80.08%] [G loss: 0.083892, mse: 0.082532]\n",
      "4060 [D loss: 0.447536, acc: 80.47%] [G loss: 0.078954, mse: 0.077594]\n",
      "4070 [D loss: 0.481938, acc: 75.78%] [G loss: 0.080140, mse: 0.078850]\n",
      "4080 [D loss: 0.434824, acc: 83.59%] [G loss: 0.070790, mse: 0.069217]\n",
      "4090 [D loss: 0.468982, acc: 78.52%] [G loss: 0.080258, mse: 0.078826]\n",
      "4100 [D loss: 0.490835, acc: 76.95%] [G loss: 0.075229, mse: 0.073681]\n",
      "4110 [D loss: 0.449335, acc: 80.86%] [G loss: 0.076428, mse: 0.074902]\n",
      "4120 [D loss: 0.447719, acc: 82.03%] [G loss: 0.071036, mse: 0.069557]\n",
      "4130 [D loss: 0.471547, acc: 76.95%] [G loss: 0.079884, mse: 0.078475]\n",
      "4140 [D loss: 0.496560, acc: 75.78%] [G loss: 0.081613, mse: 0.080094]\n",
      "4150 [D loss: 0.438809, acc: 82.42%] [G loss: 0.073627, mse: 0.072202]\n",
      "4160 [D loss: 0.469738, acc: 77.73%] [G loss: 0.079012, mse: 0.077491]\n",
      "4170 [D loss: 0.477851, acc: 78.12%] [G loss: 0.088693, mse: 0.087446]\n",
      "4180 [D loss: 0.445384, acc: 79.30%] [G loss: 0.077622, mse: 0.076119]\n",
      "4190 [D loss: 0.443880, acc: 80.08%] [G loss: 0.074783, mse: 0.073342]\n",
      "4200 [D loss: 0.460229, acc: 78.91%] [G loss: 0.075069, mse: 0.073560]\n",
      "4210 [D loss: 0.469429, acc: 79.69%] [G loss: 0.084427, mse: 0.082933]\n",
      "4220 [D loss: 0.480324, acc: 78.91%] [G loss: 0.074786, mse: 0.073383]\n",
      "4230 [D loss: 0.446331, acc: 80.86%] [G loss: 0.081896, mse: 0.080311]\n",
      "4240 [D loss: 0.483070, acc: 76.56%] [G loss: 0.083389, mse: 0.081985]\n",
      "4250 [D loss: 0.513066, acc: 74.61%] [G loss: 0.070509, mse: 0.069127]\n",
      "4260 [D loss: 0.506182, acc: 75.39%] [G loss: 0.072411, mse: 0.071006]\n",
      "4270 [D loss: 0.506810, acc: 76.95%] [G loss: 0.077541, mse: 0.076116]\n",
      "4280 [D loss: 0.470819, acc: 76.56%] [G loss: 0.080550, mse: 0.079057]\n",
      "4290 [D loss: 0.476084, acc: 77.34%] [G loss: 0.084389, mse: 0.083043]\n",
      "4300 [D loss: 0.467292, acc: 77.34%] [G loss: 0.074102, mse: 0.072648]\n",
      "4310 [D loss: 0.436357, acc: 80.47%] [G loss: 0.074939, mse: 0.073527]\n",
      "4320 [D loss: 0.496920, acc: 78.52%] [G loss: 0.081822, mse: 0.080483]\n",
      "4330 [D loss: 0.482757, acc: 79.30%] [G loss: 0.075534, mse: 0.074140]\n",
      "4340 [D loss: 0.474307, acc: 79.30%] [G loss: 0.080019, mse: 0.078649]\n",
      "4350 [D loss: 0.517796, acc: 76.56%] [G loss: 0.075879, mse: 0.074509]\n",
      "4360 [D loss: 0.445447, acc: 81.25%] [G loss: 0.072057, mse: 0.070626]\n",
      "4370 [D loss: 0.495041, acc: 76.17%] [G loss: 0.076558, mse: 0.075073]\n",
      "4380 [D loss: 0.423906, acc: 80.86%] [G loss: 0.071538, mse: 0.070060]\n",
      "4390 [D loss: 0.454784, acc: 80.08%] [G loss: 0.076407, mse: 0.074914]\n",
      "4400 [D loss: 0.540938, acc: 74.61%] [G loss: 0.079603, mse: 0.078493]\n",
      "4410 [D loss: 0.477438, acc: 79.30%] [G loss: 0.073121, mse: 0.071593]\n",
      "4420 [D loss: 0.475822, acc: 80.08%] [G loss: 0.083413, mse: 0.081997]\n",
      "4430 [D loss: 0.514883, acc: 75.39%] [G loss: 0.077168, mse: 0.075848]\n",
      "4440 [D loss: 0.528850, acc: 76.17%] [G loss: 0.071279, mse: 0.069879]\n",
      "4450 [D loss: 0.485561, acc: 78.52%] [G loss: 0.074064, mse: 0.072683]\n",
      "4460 [D loss: 0.461214, acc: 76.56%] [G loss: 0.076374, mse: 0.074879]\n",
      "4470 [D loss: 0.429263, acc: 82.81%] [G loss: 0.070478, mse: 0.068996]\n",
      "4480 [D loss: 0.501892, acc: 74.61%] [G loss: 0.072803, mse: 0.071463]\n",
      "4490 [D loss: 0.455628, acc: 80.08%] [G loss: 0.079516, mse: 0.078016]\n",
      "4500 [D loss: 0.498095, acc: 75.78%] [G loss: 0.067084, mse: 0.065803]\n",
      "4510 [D loss: 0.484171, acc: 76.95%] [G loss: 0.074371, mse: 0.072924]\n",
      "4520 [D loss: 0.492507, acc: 76.17%] [G loss: 0.077349, mse: 0.075959]\n",
      "4530 [D loss: 0.453659, acc: 82.03%] [G loss: 0.074299, mse: 0.072844]\n",
      "4540 [D loss: 0.486444, acc: 76.95%] [G loss: 0.079593, mse: 0.078214]\n",
      "4550 [D loss: 0.495894, acc: 78.12%] [G loss: 0.076090, mse: 0.074643]\n",
      "4560 [D loss: 0.430415, acc: 80.08%] [G loss: 0.071923, mse: 0.070444]\n",
      "4570 [D loss: 0.435729, acc: 81.25%] [G loss: 0.078320, mse: 0.076821]\n",
      "4580 [D loss: 0.470996, acc: 80.86%] [G loss: 0.082389, mse: 0.081085]\n",
      "4590 [D loss: 0.480352, acc: 75.39%] [G loss: 0.079080, mse: 0.077740]\n",
      "4600 [D loss: 0.466191, acc: 78.12%] [G loss: 0.070969, mse: 0.069482]\n",
      "4610 [D loss: 0.431279, acc: 83.20%] [G loss: 0.075584, mse: 0.073992]\n",
      "4620 [D loss: 0.475064, acc: 78.91%] [G loss: 0.070804, mse: 0.069408]\n",
      "4630 [D loss: 0.496349, acc: 77.34%] [G loss: 0.082073, mse: 0.080796]\n",
      "4640 [D loss: 0.494024, acc: 76.56%] [G loss: 0.073123, mse: 0.071651]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4650 [D loss: 0.456974, acc: 77.73%] [G loss: 0.076247, mse: 0.074730]\n",
      "4660 [D loss: 0.447029, acc: 78.12%] [G loss: 0.078506, mse: 0.077068]\n",
      "4670 [D loss: 0.473796, acc: 79.30%] [G loss: 0.080637, mse: 0.079220]\n",
      "4680 [D loss: 0.537035, acc: 74.22%] [G loss: 0.077635, mse: 0.076269]\n",
      "4690 [D loss: 0.528634, acc: 76.17%] [G loss: 0.077473, mse: 0.076141]\n",
      "4700 [D loss: 0.401224, acc: 83.98%] [G loss: 0.078919, mse: 0.077340]\n",
      "4710 [D loss: 0.462022, acc: 77.34%] [G loss: 0.072833, mse: 0.071464]\n",
      "4720 [D loss: 0.477221, acc: 80.08%] [G loss: 0.075664, mse: 0.074093]\n",
      "4730 [D loss: 0.485937, acc: 75.78%] [G loss: 0.080506, mse: 0.079072]\n",
      "4740 [D loss: 0.511461, acc: 76.56%] [G loss: 0.076304, mse: 0.074912]\n",
      "4750 [D loss: 0.454260, acc: 77.73%] [G loss: 0.073992, mse: 0.072575]\n",
      "4760 [D loss: 0.504159, acc: 75.39%] [G loss: 0.076472, mse: 0.075035]\n",
      "4770 [D loss: 0.502164, acc: 76.17%] [G loss: 0.076245, mse: 0.074880]\n",
      "4780 [D loss: 0.475251, acc: 77.34%] [G loss: 0.071712, mse: 0.070346]\n",
      "4790 [D loss: 0.480487, acc: 78.12%] [G loss: 0.076412, mse: 0.075019]\n",
      "4800 [D loss: 0.469418, acc: 78.12%] [G loss: 0.072105, mse: 0.070684]\n",
      "4810 [D loss: 0.545640, acc: 73.83%] [G loss: 0.080321, mse: 0.078944]\n",
      "4820 [D loss: 0.485811, acc: 78.52%] [G loss: 0.074018, mse: 0.072815]\n",
      "4830 [D loss: 0.443138, acc: 78.52%] [G loss: 0.071501, mse: 0.070020]\n",
      "4840 [D loss: 0.516965, acc: 73.83%] [G loss: 0.078651, mse: 0.077336]\n",
      "4850 [D loss: 0.499748, acc: 80.47%] [G loss: 0.068925, mse: 0.067431]\n",
      "4860 [D loss: 0.478965, acc: 75.39%] [G loss: 0.074000, mse: 0.072538]\n",
      "4870 [D loss: 0.429891, acc: 81.64%] [G loss: 0.074113, mse: 0.072586]\n",
      "4880 [D loss: 0.435650, acc: 80.08%] [G loss: 0.077911, mse: 0.076476]\n",
      "4890 [D loss: 0.517003, acc: 73.83%] [G loss: 0.079288, mse: 0.077919]\n",
      "4900 [D loss: 0.539999, acc: 73.05%] [G loss: 0.078121, mse: 0.076826]\n",
      "4910 [D loss: 0.467291, acc: 76.95%] [G loss: 0.076493, mse: 0.075093]\n",
      "4920 [D loss: 0.478045, acc: 76.95%] [G loss: 0.071519, mse: 0.069969]\n",
      "4930 [D loss: 0.461899, acc: 81.25%] [G loss: 0.073835, mse: 0.072321]\n",
      "4940 [D loss: 0.430858, acc: 82.03%] [G loss: 0.076368, mse: 0.074886]\n",
      "4950 [D loss: 0.436440, acc: 82.03%] [G loss: 0.066838, mse: 0.065243]\n",
      "4960 [D loss: 0.468571, acc: 78.91%] [G loss: 0.076353, mse: 0.074866]\n",
      "4970 [D loss: 0.498511, acc: 75.39%] [G loss: 0.075778, mse: 0.074385]\n",
      "4980 [D loss: 0.465300, acc: 76.95%] [G loss: 0.076250, mse: 0.074916]\n",
      "4990 [D loss: 0.451578, acc: 78.52%] [G loss: 0.074849, mse: 0.073361]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    # ---------------------\n",
    "    #  Train Discriminator\n",
    "    # ---------------------\n",
    "\n",
    "    # Select a random batch of images\n",
    "    idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "    imgs = X_train[idx]\n",
    "\n",
    "    latent_fake = encoder.predict(imgs)\n",
    "    \n",
    "    # Here we generate the \"TRUE\" samples\n",
    "    latent_real = sample_prior(batch_size, latent_dim)\n",
    "                      \n",
    "    # Train the discriminator\n",
    "    d_loss_real = discriminator.train_on_batch(latent_real, valid)\n",
    "    d_loss_fake = discriminator.train_on_batch(latent_fake, fake)\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "    # ---------------------\n",
    "    #  Train Generator\n",
    "    # ---------------------\n",
    "\n",
    "    # Train the generator\n",
    "    g_loss = adversarial_autoencoder.train_on_batch(imgs, [imgs, valid])\n",
    "\n",
    "    # Plot the progress (every 10th epoch)\n",
    "    if epoch % 10 == 0:\n",
    "        print (\"%d [D loss: %f, acc: %.2f%%] [G loss: %f, mse: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss[0], g_loss[1]))\n",
    "    \n",
    "    # Save generated images (every sample interval, e.g. every 100th epoch)\n",
    "    if epoch % sample_interval == 0:\n",
    "        sample_images(latent_dim, decoder, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}